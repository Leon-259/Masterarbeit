\section{Nutzwertanalyse zur Vorauswahl von Machine Learning Modellen} \label{sec:Meth Nutzwert}
Für das \gls{Modul} zur Verhaltensklassifizierung wird ein Algorithmus des \glsdisp{ML}{maschinellen Lernens} benötigt. Die Auswahl eines passenden Algorithmus ist oftmals nur durch ausprobieren verschiedener Algorithmen möglich. Um Zeit einzusparen ist es Sinnvoll die Auswahl einzugrenzen \ref{sec:ML ModellSelect}. Diese Eingrenzung wird hier mittels einer Nutzwertanalyse durchgeführt. \par

Eine Nutzwertanalyse ist eine Methode zur Entscheidungsfindung, bei welcher Entscheidungsalternativen mittels einer Wertung verglichen werden \cite{Kuhnapfel.2021}. Sie ist jedoch anfällig für eine subjektive Bewertung der Entscheidungsalternativen. Aus diesem Grund wird sie hier nur für eine Eingrenzung der Auswahl verwendet und nicht für eine finale Entscheidung. Bei einer Nutzwertanalyse werden Kriterien ausgewählt, anhand derer die Alternativen bewertet werden. Diese Kriterien erhalten eine Gewichtung, welche ihre Wichtigkeit verdeutlicht. Die Gewichte werden in Prozent angegeben und ergeben kumuliert 100 \%. Für eine Alternative wird der Erfüllungsgrad in Bezug auf ein Kriterium angegeben. Für diesen Erfüllungsgrad ist eine Skala notwendig, bspw. \(\{1,2,\dots,10\}\). Diese Skala wird für alle Kriterien verwendet. Der Grad der Erfüllung wird für jede Alternative in Bezug auf alle Kriterien angegeben und Gewichtet. Die Summe der gewichteten Erfüllungswerte einer Alternative sind der Nutzwert dieser Alternative. Die Tabelle \ref{tab:bspNutzwertanalyse} zeigt beispielhaft eine Nutzwertanalyse. Mit dieser solle eine Entscheidungsfindung zu einem Autokauf stattfinden.


\begin{table}[htbp]
  \centering
  \caption{Beispiel einer Nutzwertanalyse - Entscheidungsfindung zu einem Autokauf}
  \label{tab:bspNutzwertanalyse}
  \begin{tabular}{
    l
    S[table-format=2.0]
    S[table-format=1.1]
    S[table-format=1.1]
    S[table-format=1.1]
    S[table-format=1.1]
  }
    \toprule
    {Kriterien} & {Gewichte} & {Auto A} & {Auto B} & {Auto C} \\
    \midrule
    Preis             & 50 \% & 10 & 6 & 4 \\
    Kilometeranzahl   & 30 \% &  2 & 8 & 10 \\
    Alter             & 20 \% &  4 & 8 & 8 \\
    \midrule
    Nutzwert          & {-}  & 6,4 & 7 & 6,6 \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Auswahl der Kriterien}
Zunächst werden die Kriterien vorgestellt mit denen die Nutzwertanalyse durchgeführt werden soll. Diese sind Ausgewählt in Bezug auf die Aufgabe und die Rahmenbedingungen. Als Bewertungsskala werden die Werte \(\{1,2,\dots,10\}\) verwendet. Wie die Skala an die Kriterien angesetzt wird, wird erläutert. Ebenfalls erfolgt eine Einschätzung der Wichtigkeit.\par

\textbf{Rechenaufwand:}\\
Die Anforderungen an die Rechenleistung der Algorithmen können sehr unterschiedlich sein. Rechenleistung 
steht jedoch nur begrenzt zur Verfügung. Es stehen zwei baugleiche Rechner zur Verfügung. Die Prozessoren 
habe 16 Kerne und sie haben eine maximale Taktfrequenz von 5.10 GHz. Als Grafikkarte steht nur die der 
Hauptplatine zur Verfügung. Bezogen auf die Skala bedeuten höhere Werte eine geringere Anforderung an die Rechenleistung. Das Kriterium ist als wichtig einzustufen.\par

\textbf{Datenmenge:}\\
Gerade komplexe Modelle wie im \gls{Deep Learning} benötigen eine große Datenmenge. Wie in \ref{sec:Meth Datensatz} genauer beschrieben wird, ist die Datenmenge relativ klein. Dadurch entsteht gefahr für \gls{Overfitting}, gerade bei komplexen Modellen. Bezogen auf die Skala bedeuten höhere Werte eine geringere Anforderung an die Datenmenge. Die Datenmenge hat einen sehr großen Einfluss und das Kriterium ist als sehr wichtig einzustufen. \par

\textbf{Sequenzverständins:}\\
Bei der Aufgabe handelt es sich um ein \gls{Klassifikation}[sproblem] von Zeitreihen (\ref{sec:Meth DefAufgabe}). Gutes Sequenzverständnis wäre eine nützliche Fährigkeit des Modells (\ref{sec:sequenzen ML}). Höhere Werte auf der Skala bedeuten ein besseres Sequenzverständnis. Die Fähigkeit ist sehr relevant für die Aufgabe, jedoch bestehen Möglichkeiten Sequenzen auch mit einfacheren Modellen zu verarbeiten. Aus diesem Grund ist das Kriterium nicht Entscheidend und als mittelmäßig wichtig zu gewichten. \par

\textbf{Komplexität des Anwendens:}\\
Komplexere Modelle sind i.d.R. auch komplexer Anzuwenden. Da in Bezug auf diese Arbeit, wenig Vorerfahrung mit \glsdisp{ML}{machniellem Lernen} vorhanden ist und der Zeitrahmen begrenzt ist, ist eine einfache Anwendbarkeit des Modells Vorteilhaft. Eine niedrige Komplexität erzielt auf der Skala einen höheren Wert. Aus den Zeit und Erfahrungsgründen ist das Kriterium relevant, jedoch ist das Ziel ein möglichst gutes Modell zu erhalten. Deshalb sollte das Kriterium nicht Entscheidungsgebend sein. Es wird als geringfügig wichtig gewichtet. \par

\textbf{Modellierfähigkeit:}\\
Modelle des \gls{Deep Learning} haben eine sehr hohe Modellierfähigkeit, während eine lineare Regression nur wenig komplexe Zusammenhänge modellieren kann. Es ist jedoch einfacher ein komplexeres Modell durch Regularisierung in seiner Modellierfähigkeit zu beschneiden, als ein zu einfaches Modell zu verwenden. Deshalb wird eine hohe Modellierfähigkeit mit hohen Werten der Skala bewertet. Eine sehr hohe Modellierfähigkeit sehr hoch zu Bewerten, kann sich verzerrend auf die Nutzewertanalyse auswirken. Eine sehr hohe Modellierfähigkeit kann zu \gls{Overfitting} führen, wenn die Rahmenbedingungen nicht Stimmen, wir z.B. eine ausreichende. Datenmenge. Um solchen Verzerrungen entgegen zu wirken, wird das Kriterium als geringfügig wichtig eingestuft. \par

\textbf{Geschwindigkeit der Schätzung:}\\
Da das \gls{Modul} für die Verhaltensklassifikation Echtzeitfähigkeit besitzen soll, ist die Geschwindigkeit der Ergebnisfindung des Modells wichtig. Eine hohe Geschwindigkeit erzielt hohe Wertungen auf der Skala. Die Geschwindigkeit der Schätzung ist sehr relevant für das Gesamtkonzept. Jedoch ist der Anwendungsfall nicht kritisch, weshalb keine harten Toleranzgrenzen für die Echtzeitbedingung gelten. Die Wichtigkeit ist deshalb als geringfügig wichtig eingestuft. \par

In der Tabelle \ref{tab:NWAKriterien} ist eine Übersicht der Kriterien zu sehen. Die Wichtigkeit wurde versucht zu quantisieren. Ebenfalls ist Dargestellt wie die Skalen verlaufen.


\begin{table}[ht]
    \centering
    \begin{tabular}{|l|r|r|}
         \hline
         \textbf{Kriterium}                      & \textbf{Skala \(\{1,2,\dots,10\}\)}    & \textbf{Gewichtung}\\
         \hline
         Rechenaufwand                  & hoch \dots niedrig            & 25 \%\\
         \hline
         Datenmenge                     & hoch \dots niedrig            & 34 \%\\
         \hline
         Sequenzverständnis             & niedrig \dots hoch            & 17 \%\\
         \hline
         Komplexität des Anwendens      & hoch \dots niedrig            & 8 \%\\
         \hline
         Modellierfähigkeit             & niedrig \dots hoch            & 8 \%\\
         \hline
         Schätzungsgeschwindigkeit  & niedrig \dots hoch            & 8 \%\\
         \hline
         \textbf{Summe}                 &                               & 100 \%\\
         \hline
    \end{tabular}
    \caption{Übersicht über die Kriterien für die Nutzwertanalyse und ihre Gewichte.}
    \label{tab:NWAKriterien}
\end{table}

\subsection{Abwägung der Algorithmen}
Die Algorithmen zwischen denen abgewägt werden soll, sind in \ref{sec:ML Algorithmen} vorgestellt. Sie werden hier im Bezug auf die Kriterien diskutiert. Abschließend wird die Nutzwertanalyse präsentiert und die Auswahl der Algorithmen eingegrenzt. \dubpar

\textbf{k-Means Algorithmus}\par
Der k-Means Algorithmus besitzt kein direktes Sequenzverständnis. Doch es bietet sich an Distanzmetriken für Zeitreihen einzusetzen, um eine Clusteranalyse anhand der Ähnlichkeit der zeitreihen durchzuführen. Auch die Option komprimierte \gls{Feature}[s] zu verwenden ist gegeben. Der Algorithmus ist an für sich nicht besonders Anspruchsvoll in seinem Rechenaufwand. In Kombination mit Distanzmetriken für Sequenzen, können diese dafür sorgen, dass der Aufwand steigt. Die Berechnung von solchen Distanzmetriken, wie Dynamik Time Warping kann Rechenaufwändig sein. Eine k-Means Clusteranalyse ist bereits mit einer relativ kleinen Datenmenge gut durchführbar. Die Anwendung ist prinzipiell sehr einfach, da meistens nur ein einziger \gls{Hyperparameter}, die Clusteranzahl benötigt wird. Jedoch liegt auch genau hier die Schwierigkeit. Die Clusteranzahl muss im Vorhinein bestimmt werden. Diese zu Ermitteln kann aufwendig sein. Implementationen des Algorithmus sind in verschiedenen Software-\gls{Bibliothek}[en] zu finden \cite{FabianPedregosa.2011}. Die Modellierfähigkeit ist begrenzt aus mehreren Gründen. Zum einen lernt das Modell unüberwacht, wodurch der Lernprozess nicht kontrollierbar ist. Das sorgt dafür, dass auch die Modellierung nur schwierig anzupassen ist. Zusätzlich ist das Vorgehen wenig komplex, wodurch wichtige Zusammenhänge in den \gls{Feature}[s] eventuell nicht gefunden werden. Von der Geschwindigkeit der Schätzung ist nicht davon auszugehen, dass sie problematisch seien wird.\dubpar

\textbf{Hierarchisches Clustering}\par
Das hierarchische Clustering ist in den meisten Aspekten ähnlich einzustufen wie der k-Means Algorithmus. Der Hauptunterschied liegt darin, dass die Cluster selbst gefunden werden, ohne vorweg eingestellt werden zu müssen. Das macht die Anwendung einfacher als beim k-Means Algorithmus.\dubpar

\textbf{Logistische Regression}\par
Die Logistische Regression ist, wie in \ref{sec:ML Algorithmen} dargestellt im Grunde ein lineares Modell. Das macht die Berechnung relativ einfach. Der Rechenaufwand ist dem entsprechend nicht sonderlich hoch. Jedoch ist das Modell dadurch auch nicht besonders komplex, was die Modellierfähigkeit begrenzt. Da es jedoch überwacht lernt und Regularisierung möglich ist, ist es deutlich Anpassbarer als k-Means und das hierarchische Clustering. Es ist in der Lage mit einer relativ kleinen Datenmenge auszukommen und Das Modell ist einfach anzuwenden. Auch die Dauer der Schätzung ist sehr gering zu erwarten, da die Berechnung des Ergebnisses im Kern mittels einer linearen Gleichung erfolgt. Reihenfolgen in Daten kann das Modell nicht berücksichtigen. Sequenzverständnis ist deshalb nur mit der Komprimierung der Zeitreihen möglich.\dubpar

\textbf{Support Vector Machines}\par
\acrshort{SVM}[s] basieren im Kern auch auf einem linearen Modell, wodurch sie im Bezug auf den Rechenaufwand, die Datenmenge der Komplexität des Anwendens und der Schätzungsgeschwindigkeit ähnlich sind wie die logistische Regression. Da \acrshort{SVM}[s] jedoch durch Modifikationen auch in der Lage sind nicht-lineare Modelle zu erstellen ist die Modellierfähigkeit besser als bei der logistischen Regression. Dies kann jedoch den Rechenaufwand erhöhen.\dubpar

\textbf{Random Forest}\par
Da das Random Forest Modell seine Entscheidungsbäume parallel erstellt, lässt es sich mit mehreren Prozessorkernen effizient erstellen. Da es jedoch mehrere einfache Modelle verwendet, ist der Rechenaufwand dennoch höher als bei der \acrshort{SVM} oder der linearen Regression. Dafür ist Modellierfähigkeit besser. Zu Konfigurtation des Modells sind mehr Hyperparameter einstellbar. Das macht die Anwednung etwas komplexer, als bei der SVM und der linearen Regression. In der Regel benötigt das Modell etwas mehr Daten als die linearen Modelle, jedoch ist die Modellkomplexität durch die Hyperparameter gut einstellbar, so dass eine geringe Datenmenge ausreichen kann. Die Schätzgeschwindigkeit ist höher zu erwarten als bei den linearen Modellen, da die Ergebnisfindung aus zwei Schritten besteht. Im ersten finden die einzelnen Bäume eine Lösung und im Zweiten wird die Mehrheit ermittelt. Das Sequenzverständnis ist genau so gut wie bei den linearen Modellen.\dubpar

\textbf{Gradient Boosting}\par
Gradient Boosting ist Rechenintensiver als Random Forest. Durch die sequentielle Erstellung der Bäume ist das Training nicht parallelisierbar. In der Regel ist das Modell Modellierfähiger als Random Forest. Durch das erlernen wie es Fehler korrigieren muss, wird das Modell besser auf die Daten zugeschnitten, jedoch ist das Potenzial für Overfitting dadurch höher als bei Random Forest. In den anderen Kriterien ist eine ähnlich gute Performance zu erwarten wie bei Random Forest.\dubpar

\textbf{Long Short Term Memory}\par
Der große Vorteil von LSTMs und Deep Learning für die Aufgabe ist, dass sie Sequenzen verstehen können. Dadurch muss nicht mit Informationsverlust durch Komprimierung gerechnet werden. Deep Learning Modelle sind extrem Modellierfähig, jedoch auch komplex anzuwenden, da sie viele Hyperparameter besitzen. Für einfachere Aufgaben sind jedoch auch Netzwerke möglich, die nur wenige Schichten haben, wodurch die Komplexität überschaubar bleibt. Dadurch das mehrere Schichten durchlaufen werden, ist mit einer erhöhten Dauer für die Ergebnisfindung zu rechnen. Deep Learning benötigt jede Menge Daten und viel Rechenleistung. \dubpar

\textbf{Gated Recurrent Unit}\par
Die GRU ist in den meisten Kriterien ähnlich zu bewerten wie das LSTM, aus den gleichen Gründen. Durch die reduzierte Komplexität der GRU ist mit einem etwas geringeren Rechenaufwand auszugehen. Auch die benötigte Datenmenge ist dadurch vermutlich etwas geringer.\dubpar

In der Tabelle \ref{tab:NWA} sind die Überlegungen zu den Kriterien in einer Nutzwertanalyse dargestellt. Die Deep Learing Modelle LSTM und GRU belegen die untersten beiden Ränge. Die Anwendung von Deep Learning Algorithmen sollte nicht weiter Verfolgt werden. Mit einigem Abstand folgen die Verfahren der Clusteranalyse. Jedoch Fallen die Unterschiede der Ränge 6 bis 1 nur gering aus. Da die Anwendung eines Modells, welches überwacht lernt für am Vielversprechendsten gehalten wird, wird die Auswahl auf die oberen 4 Ränge Eingegrenzt. Somit wird die Verwendung einer logistischen Regression, einer SVM, eines Random Forest und eines Gradient Boostings getestet. Vorteilhaft ist auch, dass die \gls{Python} \gls{Bibliothek} \textit{Scikit-learn} diese 4 Modelle implementiert \cite{FabianPedregosa.2011, Buitinck.2013}. Dadurch ist das vorgehen beim Anwendung der Modelle ähnlich. \textit{Scikit-learn} wird Weit verbreitet eingesetzt im Bereich des \glsdisp{ML}{maschinellen Lernens}. 

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{11pt}
\caption{Nutzwertanalyse der Modelle}
\label{tab:NWA}
\begin{tabular}{
    l
    S[table-format = 1.2]
    *{6}{S[table-format=2.0]} % Formatierung für die Zahlen ohne Dezimalstellen
    S[table-format=2.2]
    S[table-format=1.0]
  }
\toprule
Kriterien & 
\rotatebox{90}{Gewicht} & 
\rotatebox{90}{k-Means} & 
\rotatebox{90}{Hierarchisches Clustering} & 
\rotatebox{90}{Logistische Regression} & 
\rotatebox{90}{SVM} & 
\rotatebox{90}{Random Forest} & 
\rotatebox{90}{Gradient Boosting} & 
\rotatebox{90}{LSTM} & 
\rotatebox{90}{GRU}  \\
\midrule
Rechenaufwand               & 0,25 & 6 & 6 & 10 & 9  & 8 & 7 & 1  & 2 \\
Datenmenge                  & 0,34 & 9 & 9 & 9  & 9  & 8 & 8 & 1  & 2 \\
Sequenzverständnis          & 0,17 & 5 & 5 & 4  & 4  & 4 & 4 & 10 & 10 \\
Komplexität des Anwendens   & 0,08 & 5 & 7 & 7  & 7  & 6 & 6 & 2  & 2 \\
Modellierfähigkeit          & 0,08 & 1 & 1 & 5  & 6  & 7 & 8 & 10 & 10 \\
Schätzungsgeschwindigkeit   & 0,08 & 9 & 9 & 10 & 10 & 8 & 8 & 5  & 5 \\
\midrule
\addlinespace % Extra Abstand vor der letzten Zeile
Nutzwert & {--} & 6,6 & 6,8 & 8,0 & 7,8 & 7,1 & 6,9 & 3,7 & 4,2 \\
Rang & {--} & 6 & 5 & 1 & 2 & 3 & 4 & 8 & 7 \\
\bottomrule
\end{tabular}
\end{table}

