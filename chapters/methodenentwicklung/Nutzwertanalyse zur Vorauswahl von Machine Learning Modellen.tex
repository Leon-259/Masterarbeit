\section{Nutzwertanalyse zur Vorauswahl von maschinellen Lernmodellen} \label{sec:Meth Nutzwert}
Für das \gls{Modul} zur Verhaltensklassifizierung wird ein Algorithmus des \glsdisp{ML}{maschinellen Lernens} benötigt. Die Auswahl eines passenden Algorithmus ist oftmals nur durch Ausprobieren möglich. Um Zeit einzusparen, ist es sinnvoll, die Optionen einzugrenzen (\autoref{sec:ML ModellSelect}). Diese Eingrenzung wird hier mithilfe einer Nutzwertanalyse durchgeführt. \par

Eine Nutzwertanalyse ist eine Methode zur Entscheidungsfindung, bei der Entscheidungsalternativen mittels einer Wertung verglichen werden \cite{Kuhnapfel.2021}. Sie ist jedoch anfällig für subjektive Bewertungen. Aus diesem Grund wird das Verfahren hier nur für eine Eingrenzung der Auswahl verwendet und nicht für eine finale Entscheidung. Bei einer Nutzwertanalyse werden Kriterien ausgewählt, anhand derer die Alternativen bewertet werden. Diese Kriterien erhalten eine Gewichtung, die ihre Wichtigkeit verdeutlicht. Die Gewichte werden in Prozent angegeben und ergeben kumuliert 100 \%. Eine Alternative erhält für jedes Kriterium eine Angabe des Erfüllungsgrads. Die Summe der gewichteten Erfüllungsgrade ist der Nutzwert dieser Alternative. Für den Erfüllungsgrad ist eine Skala notwendig, bspw. \(\{1,2,\dots,10\}\). Diese Skala ist für alle Kriterien gleich. Die Tabelle \ref{tab:bspNutzwertanalyse} zeigt beispielhaft eine Nutzwertanalyse. Mit dieser solle eine Entscheidungsfindung zu einem Autokauf stattfinden.


\begin{table}[htbp]
  \centering
  \caption{Beispiel einer Nutzwertanalyse - Entscheidungsfindung zu einem Autokauf}
  \label{tab:bspNutzwertanalyse}
  \begin{tabular}{
    l
    S[table-format=2.0]
    S[table-format=1.1]
    S[table-format=1.1]
    S[table-format=1.1]
    S[table-format=1.1]
  }
    \toprule
    {Kriterien} & {Gewichte} & {Auto A} & {Auto B} & {Auto C} \\
    \midrule
    Preis             & 50 \% & 10 & 6 & 4 \\
    Kilometeranzahl   & 30 \% &  2 & 8 & 10 \\
    Alter             & 20 \% &  4 & 8 & 8 \\
    \midrule
    Nutzwert          & {-}  & 6,4 & 7 & 6,6 \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Auswahl der Kriterien}
Zunächst werden die Kriterien vorgestellt, mit denen die Nutzwertanalyse durchgeführt werden soll. Diese wurden in Bezug auf die Aufgabe und Rahmenbedingungen ausgewählt. Als Bewertungsskala werden die Werte \(\{1,2,\dots,10\}\) verwendet. Wie die Skala an die Kriterien angesetzt wird, wird erläutert. Ebenfalls erfolgt eine Einschätzung der Wichtigkeit.\par

\textbf{Rechenaufwand:}\\
Die Anforderungen an die Rechenleistung der Algorithmen können sehr unterschiedlich sein und ist nur begrenzt verfügbar. Es stehen zwei baugleiche Rechner zur Verfügung. Die Prozessoren 
haben 16 Kerne mit einer maximalen Taktfrequenz von 5.10 GHz. Die Hauptplatine besitzt eine interne Grafikkarte und es ist keine zusätzliche Grafikkarte eingebaut.  Bezogen auf die Skala bedeuten höhere Werte eine geringere Anforderung an die Rechenleistung. Das Kriterium ist als wichtig einzustufen.\par

\textbf{Datenmenge:}\\
Gerade komplexe Modelle wie beim \gls{Deep Learning} benötigen eine große Datenmenge. Wie in \autoref{sec:Meth Datensatz} genauer beschrieben wird, ist die Datenmenge relativ klein. Dadurch entsteht die Gefahr von \gls{Overfitting}. Bezogen auf die Skala bedeuten höhere Werte eine geringere Anforderung an die Datenmenge. Die Datenmenge hat einen sehr großen Einfluss und das Kriterium ist als sehr wichtig einzustufen. \par

\textbf{Sequenzverständins:}\\
Bei der Aufgabe handelt es sich um ein \gls{Klassifikation}[sproblem] von Zeitreihen (\autoref{sec:Meth DefAufgabe}). Ein gutes Sequenzverständnis wäre eine nützliche Fähigkeit des Modells (\autoref{sec:sequenzen ML}). Höhere Werte auf der Skala bedeuten ein besseres Sequenzverständnis. Die Fähigkeit ist sehr relevant für die Aufgabe, jedoch bestehen andere Möglichkeiten, Sequenzen auch mit einfacheren Modellen zu verarbeiten. Aus diesem Grund ist das Kriterium nicht entscheidend und als mittelmäßig wichtig zu gewichten. \par

\textbf{Komplexität des Anwendens:}\\
Sehr modellierfähige Modelle sind i. d. R. auch komplexer anzuwenden. Da in Bezug auf diese Arbeit wenig Vorerfahrung mit \glsdisp{ML}{maschniellem Lernen} vorhanden ist und der Zeitrahmen begrenzt ist, ist eine einfache Anwendbarkeit des Modells vorteilhaft. Eine niedrige Komplexität erzielt auf der Skala einen höheren Wert. Aus den Zeit- und Erfahrungsgründen ist das Kriterium relevant, jedoch ist das Ziel ein möglichst gutes Modell zu erhalten. Deshalb sollte das Kriterium nicht ausschlaggebend sein. Es wird als geringfügig wichtig gewichtet. \par

\textbf{Modellierfähigkeit:}\\
Modelle des \gls{Deep Learning} haben eine sehr hohe Modellierfähigkeit, während eine lineare Regression nur wenig komplexe Zusammenhänge erfassen kann. Es ist jedoch einfacher, ein komplexeres Modell durch Regularisierung in seiner Modellierfähigkeit zu beschneiden, als ein zu einfaches Modell zu verwenden.Deshalb wird einer hohen Modellierfähigkeit ein hoher Wert auf der Skala zugeordnet. Jedoch kann sich eine sehr hohe Modellierfähigkeit verzerrend auf die Nutzwertanalyse auswirken. Eine sehr hohe Modellierfähigkeit kann zu \gls{Overfitting} führen, wenn die Rahmenbedingungen nicht stimmen, wie eine unzureichende Datenmenge. Um solchen Verzerrungen entgegenzuwirken, wird das Kriterium als geringfügig wichtig eingestuft. \par

\textbf{Geschwindigkeit der Schätzung:}\\
Da das \gls{Modul} für die Verhaltensklassifikation Echtzeitfähigkeit besitzen soll, ist die Geschwindigkeit der Ergebnisfindung des Modells wichtig. Eine hohe Geschwindigkeit erzielt hohe Wertungen auf der Skala. Die Geschwindigkeit der Schätzung ist sehr relevant für das Gesamtkonzept. Jedoch ist der Anwendungsfall nicht kritisch, weshalb keine harten Toleranzgrenzen für die Echtzeitbedingung gelten. Die Wichtigkeit ist daher als geringfügig wichtig eingestuft. \par

Die Tabelle 3.1 stellt eine Übersicht über die Kriterien dar. Es wurde versucht, die Wichtigkeit zu quantisieren. Ebenfalls ist dargestellt, wie die Skalen verlaufen.


\begin{table}[ht]
    \centering
    \begin{tabular}{|l|r|r|}
         \hline
         \textbf{Kriterium} & \textbf{Skala \(\{1,2,\dots,10\}\)} & \textbf{Gewichtung}\\
         \hline
         Datenmenge                     & hoch \dots niedrig            & 34 \%\\
         \hline
         Rechenaufwand                  & hoch \dots niedrig            & 25 \%\\
         \hline
         Sequenzverständnis             & niedrig \dots hoch            & 17 \%\\
         \hline
         Komplexität des Anwendens      & hoch \dots niedrig            & 8 \%\\
         \hline
         Modellierfähigkeit             & niedrig \dots hoch            & 8 \%\\
         \hline
         Schätzungsgeschwindigkeit      & niedrig \dots hoch            & 8 \%\\
         \hline
         \textbf{Summe}                 &                               & 100 \%\\
         \hline
    \end{tabular}
    \caption{Übersicht über die Kriterien für die Nutzwertanalyse und ihre Gewichte.}
    \label{tab:NWAKriterien}
\end{table}

\subsection{Abwägung der Algorithmen}
Die Algorithmen, zwischen denen abgewägt wird, sind in \autoref{sec:ML Algorithmen} vorgestellt. Sie werden hier in Bezug auf die Kriterien diskutiert. Abschließend wird die Nutzwertanalyse präsentiert und die Auswahl der Algorithmen eingegrenzt. \dubpar

\textbf{k-Means Algorithmus}\par
Der k-Means Algorithmus besitzt kein direktes Sequenzverständnis. Es ist jedoch möglich, Distanzmetriken für Zeitreihen einzusetzen, um eine Clusteranalyse anhand der Ähnlichkeit der Zeitreihen durchzuführen. Auch die Option, komprimierte \gls{Feature}[s] zu verwenden ist gegeben. Der Algorithmus ist an sich nicht besonders anspruchsvoll, mit Blick auf den Rechenaufwand. In Kombination mit Distanzmetriken für Sequenzen können diese dafür sorgen, dass der Aufwand steigt. Die Berechnung von solchen Distanzmetriken wie Dynami-Time-Warping kann rechenaufwändig sein. Eine k-Means Clusteranalyse ist bereits mit einer relativ kleinen Datenmenge gut durchführbar. Die Anwendung ist prinzipiell sehr einfach, da meistens nur ein einziger \gls{Hyperparameter}, die Clusteranzahl, benötigt wird. Jedoch liegt auch genau hier die Schwierigkeit. Die Clusteranzahl muss im Vorhinein bestimmt werden. Diese zu ermitteln kann aufwendig sein. Implementationen des Algorithmus sind in verschiedenen Software-\gls{Bibliothek}[en] zu finden \cite{FabianPedregosa.2011}. Die Modellierfähigkeit ist aus mehreren Gründen begrenzt. Zum einen lernt das Modell unüberwacht, wodurch der Lernprozess nicht kontrollierbar ist. Das sorgt dafür, dass auch die Modellierung nur schwierig anzupassen ist. Zum anderen ist das Vorgehen wenig komplex, sodass wichtige Zusammenhänge in den \gls{Feature}[s] eventuell nicht gefunden werden. Im Hinblick auf die Geschwindigkeit der Schätzung ist nicht davon auszugehen, dass sie problematisch sein wird.\dubpar

\textbf{Hierarchisches Clustering}\par
Das hierarchische Clustering ist bei den meisten Aspekten ähnlich einzustufen wie der k-Means Algorithmus. Der Hauptunterschied liegt darin, dass die Cluster selbst gefunden werden, ohne vorweg eingestellt werden zu müssen. Das macht die Anwendung einfacher als beim k-Means Algorithmus.\dubpar

\textbf{Logistische Regression}\par
Die logistische Regression ist, wie in \autoref{sec:ML Algorithmen} dargestellt, im Grunde ein lineares Modell. Das macht die Berechnung relativ einfach und unaufwändig. Jedoch ist das Modell dadurch auch nicht besonders komplex, was die Modellierfähigkeit begrenzt. Da es jedoch überwacht lernt und Regularisierung möglich ist, ist es deutlich anpassbarer als k-Means und das hierarchische Clustering. Es ist in der Lage, mit einer relativ kleinen Datenmenge auszukommen und das Modell ist einfach anzuwenden. Auch die Dauer der Schätzung ist erwartungsgemäß sehr gering, da die Berechnung des Ergebnisses im Kern mittels einer linearen Gleichung erfolgt. Reihenfolgen in Daten kann das Modell nicht berücksichtigen. Sequenzverständnis ist deshalb nur mit der Komprimierung der Zeitreihen möglich.\dubpar

\textbf{Support Vector Machines}\par
\acrshort{SVM}[s] basieren im Kern auch auf einem linearen Modell, wodurch sie in Bezug auf den Rechenaufwand, die Datenmenge, der Komplexität des Anwendens und der Schätzungsgeschwindigkeit ähnlich sind wie die logistische Regression. Da \acrshort{SVM}[s] jedoch durch Modifikationen auch in der Lage sind, nicht-lineare Modelle zu erstellen, ist die Modellierfähigkeit besser als bei der logistischen Regression. Dies kann jedoch den Rechenaufwand erhöhen.\dubpar

\textbf{Random Forest}\par
Da das Random Forest Modell seine Entscheidungsbäume parallel erstellt, lässt es sich mit mehreren Prozessorkernen effizient berechnen. Da es jedoch mehrere einfache Modelle verwendet, ist der Rechenaufwand dennoch höher als bei der \acrshort{SVM} oder der linearen Regression. Dafür ist ide Modellierfähigkeit besser. In der Konfiguration des Modells sind mehr Hyperparameter einstellbar. Das macht die Anwendung etwas komplexer. In der Regel benötigt das Modell etwas mehr Daten als die linearen Modelle, jedoch ist die Modellkomplexität durch die Hyperparameter gut einstellbar, sodass eine geringe Datenmenge ausreichen kann. Die Schätzgeschwindigkeit ist erwartungsgemäß höher als bei den linearen Modellen, da die Ergebnisfindung aus zwei Schritten besteht. Im ersten finden die einzelnen Bäume eine Lösung und im zweiten wird die Mehrheit ermittelt. Das Sequenzverständnis ist genauso gut wie bei den linearen Modellen.\dubpar

\textbf{Gradient Boosting}\par
Gradient Boosting ist rechenintensiver sowie modellierfähiger als Random Forest. Durch die sequenzielle Erstellung der Bäume ist das Training nicht parallelisierbar. Durch das Erlernen, der Fehlerkorrektur, wird das Modell besser auf die Daten zugeschnitten, jedoch ist das Potenzial für Overfitting dadurch höher als bei Random Forest. In den anderen Kriterien ist eine ähnlich gute Performance zu erwarten.\dubpar

\textbf{Long Short Term Memory}\par
Der große Vorteil von LSTMs und Deep Learning für die Aufgabe ist, dass sie Sequenzen verstehen können. Dadurch muss nicht mit Informationsverlust durch Komprimierung gerechnet werden. Deep Learning Modelle sind extrem modellierfähig, jedoch auch komplex anzuwenden, da sie viele Hyperparameter besitzen. Für einfachere Aufgaben sind jedoch auch Netzwerke möglich, die nur wenige Schichten haben, wodurch die Komplexität überschaubar bleibt. Dadurch dass mehrere Schichten durchlaufen werden, ist mit einer erhöhten Dauer für die Ergebnisfindung zu rechnen. Deep Learning benötigt jede Menge Daten und viel Rechenleistung. \dubpar

\textbf{Gated Recurrent Unit}\par
Die GRU ist in den meisten Kriterien aus den gleichen Gründen ähnlich zu bewerten wie das LSTM. Durch die reduzierte Komplexität der GRU ist von einem etwas geringeren Rechenaufwand auszugehen. Auch die benötigte Datenmenge ist dadurch vermutlich etwas geringer.\dubpar

In der Tabelle \ref{tab:NWA} sind die Überlegungen zu den Kriterien in einer Nutzwertanalyse dargestellt. Die Deep Learing Modelle LSTM und GRU belegen die untersten beiden Ränge. Die Anwendung von Deep Learning Algorithmen sollte nicht weiter verfolgt werden. Mit einigem Abstand folgen die Verfahren der Clusteranalyse. Jedoch fallen die Unterschiede der Ränge 6 bis 1 nur gering aus. Da die Anwendung eines Modells, das überwacht lernt, für am vielversprechendsten gehalten wird, wird die Auswahl auf die oberen vier Ränge eingegrenzt. Somit wird die Verwendung einer logistischen Regression, einer SVM, eines Random Forest und eines Gradient Boostings getestet. Vorteilhaft ist auch, dass die \gls{Python} \gls{Bibliothek} \textit{Scikit-learn} diese vier Modelle implementiert \cite{FabianPedregosa.2011, Buitinck.2013}. Dadurch ist das Vorgehen bei der Anwendung der Modelle ähnlich. \textit{Scikit-learn} ist weitverbreitet im Bereich des maschinellen Lernens. 

\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{11pt}
\caption{Nutzwertanalyse der Modelle}
\label{tab:NWA}
\begin{tabular}{
    l
    S[table-format = 1.2]
    *{6}{S[table-format=2.0]} % Formatierung für die Zahlen ohne Dezimalstellen
    S[table-format=2.2]
    S[table-format=1.0]
  }
\toprule
Kriterien & 
\rotatebox{90}{Gewicht} & 
\rotatebox{90}{k-Means} & 
\rotatebox{90}{Hierarchisches Clustering} & 
\rotatebox{90}{Logistische Regression} & 
\rotatebox{90}{SVM} & 
\rotatebox{90}{Random Forest} & 
\rotatebox{90}{Gradient Boosting} & 
\rotatebox{90}{LSTM} & 
\rotatebox{90}{GRU}  \\
\midrule
Datenmenge                  & 0,34 & 9 & 9 & 9  & 9  & 8 & 8 & 1  & 2 \\
Rechenaufwand               & 0,25 & 6 & 6 & 10 & 9  & 8 & 7 & 1  & 2 \\
Sequenzverständnis          & 0,17 & 5 & 5 & 4  & 4  & 4 & 4 & 10 & 10 \\
Komplexität des Anwendens   & 0,08 & 5 & 7 & 7  & 7  & 6 & 6 & 2  & 2 \\
Modellierfähigkeit          & 0,08 & 1 & 1 & 5  & 6  & 7 & 8 & 10 & 10 \\
Schätzungsgeschwindigkeit   & 0,08 & 9 & 9 & 10 & 10 & 8 & 8 & 5  & 5 \\
\midrule
\addlinespace % Extra Abstand vor der letzten Zeile
Nutzwert & {--} & 6,6 & 6,8 & 8,0 & 7,8 & 7,1 & 6,9 & 3,7 & 4,2 \\
Rang & {--} & 6 & 5 & 1 & 2 & 3 & 4 & 8 & 7 \\
\bottomrule
\end{tabular}
\end{table}

